import streamlit as st
import os
import tempfile
import datetime

# 1. ç¯å¢ƒå˜é‡é…ç½®
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
# os.environ["TAVILY_API_KEY"] = "ä½ çš„Key" # ç¡®ä¿ Key å­˜åœ¨

from dotenv import load_dotenv
# RAG ç›¸å…³
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
# Agent ç›¸å…³
from langchain_classic.agents import create_tool_calling_agent, AgentExecutor
from langchain_community.tools import TavilySearchResults
# ğŸ‘‡ æ–°å¢ï¼šæ–‡ä»¶ç®¡ç†å·¥å…·ç®±
from langchain_community.agent_toolkits import FileManagementToolkit

# åŸºç¡€ç»„ä»¶
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# ==========================================
# âš™ï¸ å…¨å±€é…ç½®ï¼šå®šä¹‰å·¥ä½œåŒºè·¯å¾„
# ==========================================
# Agent åªèƒ½åœ¨è¿™ä¸ªæ–‡ä»¶å¤¹é‡Œè¯»å†™æ–‡ä»¶ï¼Œä¿è¯å®‰å…¨
WORKSPACE_DIR = "./agent_workspace"
if not os.path.exists(WORKSPACE_DIR):
    os.makedirs(WORKSPACE_DIR)

# ==========================================
# é¡µé¢åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(page_title="è¶…çº§ AI å‘˜å·¥", page_icon="ğŸ’¼", layout="wide")
st.title("ğŸ’¼ è¶…çº§ AI å‘˜å·¥ (RAG + è”ç½‘ + å†™æ–‡ä»¶)")


# ==========================================
# æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# ==========================================

def get_llm():
    return ChatOpenAI(
        model="deepseek-chat",
        openai_api_base="https://api.deepseek.com",
        temperature=0.7
    )


# 1. æ„å»º RAG é“¾ (ä¿æŒä¸å˜)
@st.cache_resource
def create_rag_chain(file_paths):
    all_docs = []
    for path in file_paths:
        loader = PyPDFLoader(path)
        all_docs.extend(loader.load())

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(all_docs)

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ã€ä¸Šä¸‹æ–‡ã€‘å›ç­”é—®é¢˜ã€‚"
        "å¦‚æœã€ä¸Šä¸‹æ–‡ã€‘é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´ä¸çŸ¥é“ã€‚"
        "\n\nã€ä¸Šä¸‹æ–‡ã€‘: {context}"
    )
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    rag_llm = ChatOpenAI(
        model="deepseek-chat",
        openai_api_base="https://api.deepseek.com",
        temperature=0
    )
    question_answer_chain = create_stuff_documents_chain(rag_llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)
    return rag_chain


# 2. æ„å»ºå…¨èƒ½ Agent (è”ç½‘ + æ–‡ä»¶ç³»ç»Ÿ)
def create_general_agent():
    llm = get_llm()

    # A. æœç´¢å·¥å…·
    search_tool = TavilySearchResults(max_results=3)

    # B. æ–‡ä»¶ç³»ç»Ÿå·¥å…· (é™åˆ¶åœ¨ WORKSPACE_DIR ç›®å½•ä¸‹)
    # åŒ…å«ï¼šwrite_file, read_file, list_directory ç­‰å·¥å…·
    file_toolkit = FileManagementToolkit(root_dir=WORKSPACE_DIR)
    file_tools = file_toolkit.get_tools()

    # åˆå¹¶å·¥å…·é›†
    tools = [search_tool] + file_tools

    current_time = datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")

    prompt = ChatPromptTemplate.from_messages([
        (
            "system",
            f"ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½å‹ AI åŠ©æ‰‹ã€‚å½“å‰æ—¶é—´ï¼š{current_time}ã€‚\n"
            "ä½ æœ‰ä¸¤ä¸ªæ ¸å¿ƒèƒ½åŠ›ï¼š\n"
            "1. **è”ç½‘æœç´¢**ï¼šä½¿ç”¨ search å·¥å…·è·å–å®æ—¶ä¿¡æ¯ã€‚\n"
            "2. **æ–‡ä»¶ç®¡ç†**ï¼šä½¿ç”¨ write_file å·¥å…·åœ¨æœ¬åœ°å·¥ä½œåŒºåˆ›å»ºæ–‡ä»¶ï¼Œç”¨ list_directory æŸ¥çœ‹æ–‡ä»¶ã€‚\n"
            "âš ï¸ åªæœ‰å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚'ç”ŸæˆæŠ¥å‘Š'ã€'ä¿å­˜æ–‡ä»¶'æˆ–'å†™ä»£ç 'æ—¶ï¼Œæ‰ä½¿ç”¨æ–‡ä»¶å·¥å…·ã€‚\n"
            "âš ï¸ é»˜è®¤å°†æ–‡ä»¶ä¿å­˜ä¸º Markdown (.md) æˆ– Python (.py) æ ¼å¼ã€‚"
        ),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ])

    agent = create_tool_calling_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    return agent_executor


# 3. æ™®é€šé—²èŠ
def create_chat_chain():
    current_time = datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
    prompt = ChatPromptTemplate.from_messages([
        ("system", f"ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹ã€‚å½“å‰æ—¶é—´ï¼š{current_time}ã€‚"),
        ("human", "{input}"),
    ])
    llm = get_llm()
    chain = prompt | llm | StrOutputParser()
    return chain


# ==========================================
# ä¾§è¾¹æ ï¼šæ§åˆ¶ä¸­å¿ƒ & æ–‡ä»¶æµè§ˆå™¨
# ==========================================
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶å°")

    # --- RAG éƒ¨åˆ† ---
    st.subheader("ğŸ“š çŸ¥è¯†åº“")
    uploaded_files = st.file_uploader("ä¸Šä¼  PDF", type="pdf", accept_multiple_files=True)
    if uploaded_files:
        if st.button("ğŸ”„ æ„å»ºçŸ¥è¯†åº“"):
            st.session_state.is_processing = True

    # --- Agent å¼€å…³ ---
    st.subheader("ğŸ¤– æ™ºèƒ½ä½“èƒ½åŠ›")
    # å¦‚æœæ²¡ä¼ æ–‡ä»¶ï¼Œå¯ä»¥è®©ç”¨æˆ·é€‰æ‹©å¼€å¯ Agent æ¨¡å¼
    enable_agent = st.toggle("å¼€å¯å…¨èƒ½ Agent (è”ç½‘+å†™æ–‡ä»¶)", value=False)

    st.divider()

    # --- ğŸ“‚ æ–‡ä»¶æµè§ˆå™¨ (æ–°åŠŸèƒ½) ---
    st.subheader("ğŸ“‚ æœ¬åœ°å·¥ä½œåŒº")
    st.caption(f"è·¯å¾„: {WORKSPACE_DIR}")

    # åˆ·æ–°æ–‡ä»¶åˆ—è¡¨æŒ‰é’®
    if st.button("ğŸ”„ åˆ·æ–°æ–‡ä»¶åˆ—è¡¨"):
        st.rerun()

    # åˆ—å‡ºå·¥ä½œåŒºçš„æ‰€æœ‰æ–‡ä»¶
    try:
        files = os.listdir(WORKSPACE_DIR)
        if not files:
            st.info("æš‚æ— æ–‡ä»¶")
        else:
            for f in files:
                file_path = os.path.join(WORKSPACE_DIR, f)
                # ç®€å•çš„ä¸‹è½½/æŸ¥çœ‹é€»è¾‘
                with open(file_path, "rb") as file:
                    st.download_button(
                        label=f"â¬‡ï¸ ä¸‹è½½ {f}",
                        data=file,
                        file_name=f,
                        mime="text/plain"
                    )
    except Exception as e:
        st.error(f"æ— æ³•è¯»å–å·¥ä½œåŒº: {e}")

    st.divider()
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå†å²"):
        st.session_state.messages = []
        st.session_state.rag_chain = None
        st.rerun()

# ==========================================
# ä¸»é€»è¾‘
# ==========================================

# åˆå§‹åŒ– Session
if "messages" not in st.session_state:
    st.session_state.messages = []
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

# RAG å¤„ç†
if getattr(st.session_state, 'is_processing', False):
    with st.spinner("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“..."):
        temp_paths = []
        for file in uploaded_files:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(file.read())
                temp_paths.append(tmp.name)
        st.session_state.rag_chain = create_rag_chain(temp_paths)
        st.success("âœ… çŸ¥è¯†åº“å°±ç»ªï¼")
        st.session_state.is_processing = False

# æ¨¡å¼åˆ¤æ–­
current_mode = "chat"
if st.session_state.rag_chain:
    current_mode = "rag"
elif enable_agent:
    current_mode = "agent"  # Agent æ¨¡å¼ = æœç´¢ + æ–‡ä»¶æ“ä½œ

# çŠ¶æ€æ˜¾ç¤º
if current_mode == "rag":
    st.info("ğŸŸ¢ æ¨¡å¼ï¼š**çŸ¥è¯†åº“é—®ç­”** (RAG)")
elif current_mode == "agent":
    st.success("ğŸŒ æ¨¡å¼ï¼š**å…¨èƒ½ Agent** (è”ç½‘æœç´¢ + æ–‡ä»¶è¯»å†™)")
else:
    st.caption("ğŸ”µ æ¨¡å¼ï¼š**è‡ªç”±é—²èŠ** (Chat)")

# å†å²å›æ˜¾
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if "sources" in msg:
            with st.expander("ğŸ“– æ¥æº"):
                for s in msg["sources"]:
                    st.markdown(f"**P{s['page']}**: {s['content']}...")

# è¾“å…¥å¤„ç†
if prompt := st.chat_input("æŒ‡ä»¤..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("AI æ­£åœ¨æ‰§è¡Œä»»åŠ¡..."):

            # 1. RAG æ¨¡å¼
            if current_mode == "rag":
                response = st.session_state.rag_chain.invoke({"input": prompt})
                answer = response["answer"]
                sources = [{"page": d.metadata.get("page", 0) + 1, "content": d.page_content[:50]} for d in
                           response["context"]]
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer, "sources": sources})

            # 2. Agent æ¨¡å¼ (æœç´¢ + å†™æ–‡ä»¶)
            elif current_mode == "agent":
                agent = create_general_agent()
                # Agent çš„è¾“å‡ºé€šå¸¸åŒ…å«æ‰§è¡Œè¿‡ç¨‹ï¼Œæˆ‘ä»¬å– output
                response = agent.invoke({"input": prompt})
                answer = response["output"]
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

                # ğŸ‰ å¦‚æœç”Ÿæˆäº†æ–‡ä»¶ï¼Œè‡ªåŠ¨æç¤ºåˆ·æ–°ä¾§è¾¹æ 
                if "write_file" in str(response):  # ç®€å•åˆ¤æ–­æ—¥å¿—é‡Œæœ‰æ²¡æœ‰è°ƒç”¨å†™æ–‡ä»¶
                    st.toast("âœ… æ–‡ä»¶å·²ç”Ÿæˆï¼è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ æŸ¥çœ‹ã€‚", icon="ğŸ“‚")

            # 3. é—²èŠæ¨¡å¼
            else:
                chat = create_chat_chain()
                response = chat.invoke({"input": prompt})
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})